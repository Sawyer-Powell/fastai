{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7977d5-aea9-47ac-a8e2-7782776ef714",
   "metadata": {},
   "source": [
    "### Messing with decision trees\n",
    "\n",
    "Reference: https://developers.google.com/machine-learning/decision-forests/growing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053547ac-0d42-474b-ad02-637834976ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e879b0-f1af-4e70-a982-eb50ac655536",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"data/titanic/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "826a8513-6baa-4d22-929a-ff37db1511cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = pd.read_csv(f'{DATA}train.csv')\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f618efe5-c358-441e-88a1-69e9ae0d5be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18890814558058924"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "males = train_raw.Sex == \"male\"\n",
    "males = train_raw[males]\n",
    "females = train_raw.Sex != \"male\"\n",
    "females = train_raw[females]\n",
    "np.average(males.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed4e7475-9a6c-42d3-b1f5-09cfbf485393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7420382165605095"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(females.Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b2a53-b3b6-411c-9912-b3fa361c1ad6",
   "metadata": {},
   "source": [
    "Alright, let's make a decision tree. Let's take the sex as the first feature we will look into. Breaking up the data in this scenario\n",
    "is pretty trivial since sex can only take two values in this dataset. Let's look into the basic principles of the approach behind decision trees.\n",
    "\n",
    "The idea behind decision trees is that we want to create a hierarchy of features within a dataset, that allow us to describe, and better predict,\n",
    "an outcome given some set of features about a datapoint. Let's take the example of the Titanic dataset. I want to, given the data provided above, predict\n",
    "whether a person will survive or die. Using just information about sex, by using a naive approach, I'm going to make a simple model:\n",
    "\n",
    "```\n",
    "if male => will die\n",
    "else => will live\n",
    "```\n",
    "\n",
    "In this scenario, i'm just rounding the probability that a lady would die vs. a male. This model is crude, but, in the case of males, will be correct about 3/4 of the time. To make it more sophisticated, I can start nesting conditions. This nesting allows us to better capture the complexity of the data within our model. Nesting, in this case, could mean something like \"is male and age is greater than 30\". Conditions for now are always true or false, with the nesting resulting in a binary tree. The leafs of our tree are the probability of our dependent variable being the positive case given the precedeing conditions (i.e. probability that they will live).\n",
    "\n",
    "An important part of decision trees is determining the order and structure of the hierarchy, since all we start out with are a list of features that we're interested in. A good measurement fo how good a feature is for predicting the dependent variable is by measuring the information gain that feature provides. How do we measure information gain? What does that even mean? Let's just start with the intuition that an increase in information results in an increase in heterogeneity. Information allows us to separate data. More information is equivalent to saying less entropy. The big idea is that entropy makes data hard to distinguish, information allows us to distinguish it. Like putting on a new pair of glasses.\n",
    "\n",
    "### Measuring entropy\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "R(X) & = & \\frac{\\lvert \\{ x | x \\in X \\ \\textrm{and} \\ x = \\mathrm{pos}  \\} \\rvert}{\\lvert X \\rvert} \\\\[12pt]\n",
    "H(X) & = & - p \\log p - (1 - p) \\log (1-p) \\ \\textrm{with} \\ p = R(X)\\\\[12pt]\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "$R(X)$ is the proportion of the data in X that takes on the positive condition in the dependent variable.\n",
    "$H(X)$ is the Shannon Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe1952c4-9657-4186-9606-b39b66f2d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the positive condition as being alive :)\n",
    "# Let's find the proportion of males that ended up alive, and let's do the same for females\n",
    "\n",
    "r_male = np.average(males.Survived)\n",
    "r_female = np.average(females.Survived)\n",
    "\n",
    "# Let's then calculate the entropy from these proportions\n",
    "\n",
    "def entropy(p):\n",
    "    return -p * np.log(p) - (1 - p) * np.log(1 - p)\n",
    "\n",
    "h_male = entropy(r_male)\n",
    "h_female = entropy(r_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c9a5865-dfa1-48e1-9c1b-bba4155951f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also calculate the baseline entropy of the data\n",
    "\n",
    "r_base = np.average(train_raw.Survived)\n",
    "h_base = entropy(r_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24b21146-85d0-406a-9524-d044a9216440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6659119735267652, 0.5709141922481396, 0.4846358858279689)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_base, h_female, h_male"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be9cf40-9712-4344-a3b8-d5bdd3caae39",
   "metadata": {},
   "source": [
    "## Measuring information gain\n",
    "\n",
    "Just take the base entropy, and subtract the weighted average of the entropies from the two splits.\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "IG(D,T,F) & = & H(D) - \\frac {\\lvert T\\rvert} {\\lvert D \\rvert } H(T) - \\frac {\\lvert F \\rvert} {\\lvert D \\rvert } H(F)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "IG -> Information Gained\n",
    "\n",
    "Where $D$ represents the base set of the dependent variable, not filtered based on a feature's split (e.g. male vs. female for the sex feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4a24a7e-3437-462c-be4c-f4a960e09b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15087048925218174"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig = h_base - (males.size/train_raw.size) * h_male - (females.size/train_raw.size) * h_female\n",
    "ig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd73572-af04-4ede-973a-0f925530655d",
   "metadata": {},
   "source": [
    "### Dang!\n",
    "\n",
    "Now we have a way to evaluate how good a split of the data is based on a feature! Big IG means more good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d95871-2d30-4614-8066-b623515474f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cef5c8-bf46-496a-90ee-c866845d2a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
